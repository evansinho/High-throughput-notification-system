import { Injectable, Logger } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import { EmbeddingResult } from '../interfaces/vector.interface';

/**
 * Embedding Service - Generates vector embeddings for text
 *
 * NOTE: This is a simplified implementation. In production, you would use:
 * - OpenAI's text-embedding-ada-002 or text-embedding-3-small
 * - Voyage AI embeddings
 * - Local models via transformers.js or sentence-transformers
 * - Anthropic + Voyage AI integration
 */
@Injectable()
export class EmbeddingService {
  private readonly logger = new Logger(EmbeddingService.name);
  private readonly vectorSize = 1536; // Standard embedding size
  private readonly model = 'mock-embedding-model-v1';

  constructor(configService: ConfigService) {
    // ConfigService available for future use (e.g., OpenAI API key)
    const env = configService.get('app.nodeEnv');
    this.logger.log('Embedding Service initialized (mock mode)');
    this.logger.log(`Environment: ${env}`);
  }

  /**
   * Generate embedding for text
   *
   * MOCK IMPLEMENTATION: Returns deterministic vectors based on text hash
   * Replace with real embedding model in production
   */
  async generateEmbedding(text: string): Promise<EmbeddingResult> {
    try {
      // In production, call actual embedding API:
      // - OpenAI: await openai.embeddings.create({ input: text, model: 'text-embedding-ada-002' })
      // - Voyage AI: await voyageai.embed({ texts: [text], model: 'voyage-2' })
      // - Local: Use transformers.js or similar

      const embedding = this.mockEmbedding(text);

      return {
        embedding,
        model: this.model,
        dimensions: this.vectorSize,
      };
    } catch (error) {
      const errorMessage =
        error instanceof Error ? error.message : 'Unknown error';
      this.logger.error(`Failed to generate embedding: ${errorMessage}`);
      throw error;
    }
  }

  /**
   * Batch generate embeddings for multiple texts
   */
  async generateEmbeddings(texts: string[]): Promise<EmbeddingResult[]> {
    try {
      // In production, use batch API for efficiency
      const embeddings = await Promise.all(
        texts.map((text) => this.generateEmbedding(text)),
      );

      return embeddings;
    } catch (error) {
      const errorMessage =
        error instanceof Error ? error.message : 'Unknown error';
      this.logger.error(`Failed to generate batch embeddings: ${errorMessage}`);
      throw error;
    }
  }

  /**
   * MOCK: Generate deterministic embedding based on text content
   * This is for demo/testing purposes only
   *
   * Real embeddings would be generated by trained neural networks
   */
  private mockEmbedding(text: string): number[] {
    // Simple hash-based deterministic vector
    // In real implementation, this would be a neural network
    const hash = this.simpleHash(text);
    const embedding = new Array(this.vectorSize);

    // Generate pseudo-random but deterministic values
    for (let i = 0; i < this.vectorSize; i++) {
      const seed = hash + i;
      // Simple linear congruential generator
      const value = ((seed * 1103515245 + 12345) % 2147483648) / 2147483648;
      // Normalize to [-1, 1]
      embedding[i] = value * 2 - 1;
    }

    // Normalize to unit vector (L2 norm = 1)
    const norm = Math.sqrt(embedding.reduce((sum, val) => sum + val * val, 0));

    return embedding.map((val) => val / norm);
  }

  /**
   * Simple string hash function
   */
  private simpleHash(str: string): number {
    let hash = 0;
    for (let i = 0; i < str.length; i++) {
      const char = str.charCodeAt(i);
      hash = (hash << 5) - hash + char;
      hash = hash & hash; // Convert to 32bit integer
    }
    return Math.abs(hash);
  }

  /**
   * Calculate cosine similarity between two embeddings
   */
  cosineSimilarity(a: number[], b: number[]): number {
    if (a.length !== b.length) {
      throw new Error('Embeddings must have the same length');
    }

    let dotProduct = 0;
    let normA = 0;
    let normB = 0;

    for (let i = 0; i < a.length; i++) {
      dotProduct += a[i] * b[i];
      normA += a[i] * a[i];
      normB += b[i] * b[i];
    }

    return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
  }

  /**
   * Get embedding dimensions
   */
  getDimensions(): number {
    return this.vectorSize;
  }

  /**
   * Get model information
   */
  getModelInfo(): { name: string; dimensions: number } {
    return {
      name: this.model,
      dimensions: this.vectorSize,
    };
  }
}

/**
 * PRODUCTION IMPLEMENTATION NOTES:
 *
 * Option 1: OpenAI Embeddings
 * ```typescript
 * import { OpenAI } from 'openai';
 *
 * const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
 * const response = await openai.embeddings.create({
 *   input: text,
 *   model: 'text-embedding-ada-002', // 1536 dimensions, $0.10 per 1M tokens
 *   // or 'text-embedding-3-small' // 1536 dimensions, $0.02 per 1M tokens
 * });
 * return response.data[0].embedding;
 * ```
 *
 * Option 2: Voyage AI (Anthropic recommended)
 * ```typescript
 * import { VoyageAIClient } from '@voyageai/voyage';
 *
 * const voyage = new VoyageAIClient({ apiKey: process.env.VOYAGE_API_KEY });
 * const response = await voyage.embed({
 *   texts: [text],
 *   model: 'voyage-2', // 1024 dimensions
 * });
 * return response.embeddings[0];
 * ```
 *
 * Option 3: Local Model (no API costs, but slower)
 * ```typescript
 * import { pipeline } from '@xenova/transformers';
 *
 * const extractor = await pipeline('feature-extraction',
 *   'Xenova/all-MiniLM-L6-v2'); // 384 dimensions
 * const output = await extractor(text, { pooling: 'mean', normalize: true });
 * return Array.from(output.data);
 * ```
 */
